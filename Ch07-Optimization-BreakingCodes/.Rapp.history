summary(nn_predict_test)
rm(list = ls())#
#Load required packages and read data#
library(data.table)#
library(stringr)#
library(plyr)#
library(caret)#
train <- fread(input = '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/data/train.csv')#
train <- as.data.frame(train)#
test <- fread('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/data/test.csv')#
test <- as.data.frame(test)#
#
#FEATURE ENGG#
#Create new column that computes family size#
all$familySize <- all$SibSp + all$Parch#
#
#Extract salutation from name column#
all$firstMiddleName <- sapply(strsplit(all$Name, ','), '[', 2)#
all$Salutation <- sapply(strsplit(all$firstMiddleName, '\\.'), '[', 1)#
all$Salutation <- str_trim(all$Salutation, side = 'both')#
#Set all rare salutations (count < 10) as 'Other'#
all$Salutation[all$Salutation %in% c('Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Mlle', 'Mme', 'Ms', 'Other', 'Rev', 'Sir', 'the Countess', 'Capt')] <- 'Other'#
#MISSING VALUE IMPUTATION#
#Impute port of embarkation with most frequent value#
table(all$Embarked) #S is most frequent value#
all$Embarked[all$Embarked == ""] <- 'S'#
all$Embarked <- factor(all$Embarked) #drop ununsed level
rm(list = ls())#
#Load required packages and read data#
library(data.table)#
library(stringr)#
library(plyr)#
library(caret)#
train <- fread(input = '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/data/train.csv')#
train <- as.data.frame(train)#
test <- fread('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/data/test.csv')#
test <- as.data.frame(test)#
#
#combine train and test data sets into one#
test$Survived <- NA#
all <- rbind(train, test)#
#
#FEATURE ENGG#
#Create new column that computes family size#
all$familySize <- all$SibSp + all$Parch#
#
#Extract salutation from name column#
all$firstMiddleName <- sapply(strsplit(all$Name, ','), '[', 2)#
all$Salutation <- sapply(strsplit(all$firstMiddleName, '\\.'), '[', 1)#
all$Salutation <- str_trim(all$Salutation, side = 'both')#
#Set all rare salutations (count < 10) as 'Other'#
all$Salutation[all$Salutation %in% c('Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Mlle', 'Mme', 'Ms', 'Other', 'Rev', 'Sir', 'the Countess', 'Capt')] <- 'Other'#
#MISSING VALUE IMPUTATION#
#Impute port of embarkation with most frequent value#
table(all$Embarked) #S is most frequent value#
all$Embarked[all$Embarked == ""] <- 'S'#
all$Embarked <- factor(all$Embarked) #drop ununsed level
str(all)
ageTree <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + familySize + Salutation, data = all[!is.na(all$Age)], method = 'anova' )
library(rpart)
library(rpart.plot)
ageTree <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + familySize + Salutation, data = all[!is.na(all$Age)], method = 'anova' )
ageTree <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + familySize + Salutation, data = all[!is.na(all$Age), ], method = 'anova' )
rpart.plot(ageTree, type = 4)
all$Age[is.na(all$Age)] <- predict(ageTree, newdata = all[is.na(all$Age), ])
str(all)
fareTree <- rpart(Fare ~ Pclass + Sex + SibSp + Parch + Age + Embarked + familySize + Salutation, data = all[!is.na(all$Fare), ], method = 'anova')
all$Fare[is.na(all$Fare)] <- predict(fareTree, newdata = all[!is.na(all$Fare), ])
all$Fare[is.na(all$Fare)] <- predict(fareTree, newdata = all[is.na(all$Fare), ])
table(is.na(all))
write.csv(train, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv', row.names = FALSE)
write.csv(test, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/test.csv', row.names = FALSE)
colnames(all)
#Remove columns that would not be used#
all$Name <- NULL#
all$Ticket <- NULL#
all$Cabin <- NULL#
all$firstMiddleName <- NULL
colnames(all)
rm(list = ls())#
#Load required packages and read data#
library(data.table)#
library(stringr)#
library(plyr)#
library(caret)#
library(rpart)#
library(rpart.plot)#
train <- fread(input = '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/data/train.csv')#
train <- as.data.frame(train)#
test <- fread('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/data/test.csv')#
test <- as.data.frame(test)#
#
#combine train and test data sets into one#
test$Survived <- NA#
all <- rbind(train, test)#
#
#FEATURE ENGG#
#Create new column that computes family size#
all$familySize <- all$SibSp + all$Parch#
#
#Extract salutation from name column#
all$firstMiddleName <- sapply(strsplit(all$Name, ','), '[', 2)#
all$Salutation <- sapply(strsplit(all$firstMiddleName, '\\.'), '[', 1)#
all$Salutation <- str_trim(all$Salutation, side = 'both')#
#Set all rare salutations (count < 10) as 'Other'#
all$Salutation[all$Salutation %in% c('Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Mlle', 'Mme', 'Ms', 'Other', 'Rev', 'Sir', 'the Countess', 'Capt')] <- 'Other'#
#
#MISSING VALUE IMPUTATION#
#Impute port of embarkation with most frequent value#
table(all$Embarked) #S is most frequent value#
all$Embarked[all$Embarked == ""] <- 'S'#
all$Embarked <- factor(all$Embarked) #drop ununsed level#
#
#Impute missing values of age based on decision tree computed on all other available variables#
ageTree <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + familySize + Salutation, data = all[!is.na(all$Age), ], method = 'anova' )#
all$Age[is.na(all$Age)] <- predict(ageTree, newdata = all[is.na(all$Age), ])#
#
#Impute missing values of fare based on decision tree compute on all other available variables#
fareTree <- rpart(Fare ~ Pclass + Sex + SibSp + Parch + Age + Embarked + familySize + Salutation, data = all[!is.na(all$Fare), ], method = 'anova')#
all$Fare[is.na(all$Fare)] <- predict(fareTree, newdata = all[is.na(all$Fare), ])#
#
#Remove columns that would not be used#
all$Name <- NULL#
all$Ticket <- NULL#
all$Cabin <- NULL#
all$firstMiddleName <- NULL#
#split train and test data sets and write them into files#
train <- subset(all, !is.na(all$Survived))#
test <- subset(all, is.na(all$Survived))#
test$Survived <- NULL#
#
#write new datasets into train and test files#
write.csv(train, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv', row.names = FALSE)#
write.csv(test, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/test.csv', row.names = FALSE)
colnames(train)
library(glmnet)
install.packages('glmnet')
library(glmnet)
cv.glmnet(x = train[, !(colnames(train) %in% c('PassengerId', 'Survived'))], y = train[, c('Survived')], type.measure = 'class', nfolds = 10)
colnames(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])
cv.glmnet(x = x_mat, y = train$Survived, type.measure = 'class', nfolds = 10)
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')
table(is.na(train))
set.seed(10)
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])
table(is.na(x_mat))
cv.glmnet(x = x_mat, y = train$Survived, type.measure = 'class', nfolds = 10)
str(train)
str(model.matrix(~train))
str(model.matrix(~train$Sex))
head(model.matrix(~train$Sex))
head(model.matrix(~train$Sex - 1))
rm(list = ls())#
library(glmnet)#
library(caret)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
#
#Combine train and test datasets#
test$Survived <- NA#
all <- rbind(train, test)
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE)
fullData <- predict(dummy, newdata = all)
str(fullData)
891 + 418
head(fullData)
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE, fullRank = TRUE)
fullData <- predict(dummy, newdata = all)
head(fullData)
nrow(fullData)
dummyData <- predict(dummy, newdata = all)
fullData <- cbind(dummyData, all$Survived)
str(fullData)
summary(fullData$PassengerId)
fullData <- as.data.frame(cbind(dummyData, all$Survived))
str(fullData)
colnames(train)
fullData <- as.data.frame(cbind(dummyData, Survived = all$Survived))
str(fullData)
#Name: LogitWithLasso.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: glmnet, caret#
#
rm(list = ls())#
library(glmnet)#
library(caret)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
#
#Combine train and test datasets#
test$Survived <- NA#
all <- rbind(train, test)#
#
#Create dummy variables#
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE, fullRank = TRUE)#
dummyData <- predict(dummy, newdata = all)#
fullData <- as.data.frame(cbind(dummyData, Survived = all$Survived))#
#
#Split train and test data sets#
train <- subset(fullData, !is.na(fullData$Survived))#
test <- subset(fullData, is.na(fullData$Survived))#
test$Survived <- NULL
set.seed(10)
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])
cv.glmnet(x = x_mat, y = train$Survived, type.measure = 'class', nfolds = 10)
str(train)
#Name: LogitWithLasso.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: glmnet, caret#
#
rm(list = ls())#
library(glmnet)#
library(caret)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
#
#Combine train and test datasets#
test$Survived <- NA#
all <- rbind(train, test)#
#
#Create dummy variables#
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE, fullRank = TRUE)#
dummyData <- predict(dummy, newdata = all)#
fullData <- as.data.frame(cbind(dummyData, Survived = all$Survived))#
#
#Split train and test data sets#
train <- subset(fullData, !is.na(fullData$Survived))#
test <- subset(fullData, is.na(fullData$Survived))#
test$Survived <- NULL#
train$Survived <- as.factor(train$Survived)
set.seed(10)
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])
cv.glmnet(x = x_mat, y = train$Survived, type.measure = 'class', nfolds = 10)
cv.glmnet(x = x_mat, y = train$Survived, type.measure = 'auc', nfolds = 10)
#Name: LogitWithLasso.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: glmnet, caret#
#
rm(list = ls())#
library(glmnet)#
library(caret)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
#
#Combine train and test datasets#
test$Survived <- NA#
all <- rbind(train, test)#
#
#Create dummy variables#
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE, fullRank = TRUE)#
dummyData <- predict(dummy, newdata = all)#
fullData <- as.data.frame(cbind(dummyData, Survived = all$Survived))#
#
#Split train and test data sets#
train <- subset(fullData, !is.na(fullData$Survived))#
test <- subset(fullData, is.na(fullData$Survived))#
test$Survived <- NULL
set.seed(10)
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])
cv.glmnet(x = x_mat, y = train$Survived, type.measure = 'auc', nfolds = 10)
#Name: LogitWithLasso.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: glmnet, caret#
#
rm(list = ls())#
library(glmnet)#
library(caret)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
#
#Combine train and test datasets#
test$Survived <- NA#
all <- rbind(train, test)#
#
#Create dummy variables#
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE, fullRank = TRUE)#
dummyData <- predict(dummy, newdata = all)#
fullData <- as.data.frame(cbind(dummyData, Survived = all$Survived))#
#
#Split train and test data sets#
train <- subset(fullData, !is.na(fullData$Survived))#
test <- subset(fullData, is.na(fullData$Survived))#
test$Survived <- NULL#
train$Survived <- as.factor(train$Survived)#
#
#Find optimal value of lambda#
set.seed(10)#
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])
str(train)
cv.glmnet(x = x_mat, y = train$Survived, family = 'binomial', type.measure = 'class', nfolds = 10)
lassoModel <- glmnet(x = x_mat, y = train$Survived, family = 'binomial', alpha = 1, lambda = 0.0009162309)
summary(lassoModel)
lassoModel
lassoModel <- glmnet(x = x_mat, y = train$Survived, family = 'binomial', alpha = 1)
lassoModel
predictions <- predict(lassoModel, newdata = test)
x_mat_test <- as.matrix(test[, colnames(test) != 'PassengerId'])
lassoPredictions <- predict(lassoModel, newdata = x_mat_test)
lassoPredictions <- predict(lassoModel, newdata = x_mat_test, type = 'response')
lassoPredictions <- predict(lassoModel, x_mat_test, type = 'response')
summary(lasspoPredictions)
summary(lassoPredictions)
lassoModel
lassoModel <- glmnet(x = x_mat, y = train$Survived, family = 'binomial', alpha = 1, lambda = 0.0009162309)
print(lassoModel)
x_mat_test <- as.matrix(test[, colnames(test) != 'PassengerId'])
lassoPredictions <- predict(lassoModel, x_mat_test, type = 'response')
summary(lassoPredictions)
lassoOutput <- ifelse(lassoPredictions < 0.5, 0, 1)
table(lassoOutput)
str(lassoPredictions)
str(x_mat_test)
str(test)
test <- subset(fullData, is.na(fullData$Survived))
str(test)
table(is.na(fullData$Survived))
table(is.na(all$Survived))
#Name: LogitWithLasso.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: glmnet, caret#
#
rm(list = ls())#
library(glmnet)#
library(caret)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/test.csv')#
#
#Combine train and test datasets#
test$Survived <- NA#
all <- rbind(train, test)#
#
#Create dummy variables#
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE, fullRank = TRUE)#
dummyData <- predict(dummy, newdata = all)#
fullData <- as.data.frame(cbind(dummyData, Survived = all$Survived))#
#
#Split train and test data sets#
train <- subset(fullData, !is.na(fullData$Survived))#
test <- subset(fullData, is.na(fullData$Survived))#
test$Survived <- NULL#
train$Survived <- as.factor(train$Survived)#
#
#Find optimal value of lambda#
set.seed(10)#
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])#
cv.glmnet(x = x_mat, y = train$Survived, family = 'binomial', type.measure = 'class', nfolds = 10)
#Build logistic regression with lasso #
lassoModel <- glmnet(x = x_mat, y = train$Survived, family = 'binomial', alpha = 1, lambda = 0.0009162309)#
print(lassoModel)
x_mat_test <- as.matrix(test[, colnames(test) != 'PassengerId'])#
lassoPredictions <- predict(lassoModel, x_mat_test, type = 'response')#
summary(lassoPredictions)
str(lassoPredictions)
lassoOutput <- ifelse(lassoPredictions < 0.5, 0, 1)
table(lassoOutput)
str(test)
write.csv(submission, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/submission/logitWithLasso.csv', row.names = FALSE)
submission <- data.frame(PassengerId = test$PassengerId, Survived = lassoOutput)
write.csv(submission, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/submission/logitWithLasso.csv', row.names = FALSE)
#Name: LogisticRegression.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: base#
#
rm(list = ls())#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
#
#Create logistic regression model#
logModel <- glm(Survived ~ . -PassengerId, data = train, family = binomial)#
summary(logModel)#
#
#Predict output#
glmProbability <- predict(logModel, newdata = test)#
summary(glmProbability)#
#
glmOutput <- ifelse(glmProbability < 0.5, 0, 1)#
table(glmOutput)#
#
#Create submission file#
submission <- data.frame(PassengerId = test$PassengerId, Survived = glmOutput)#
write.csv(submission, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/submission/logisticRegression.csv', row.names = FALSE)
#Name: LogisticRegression.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: base#
#
rm(list = ls())#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/test.csv')#
#
#Create logistic regression model#
logModel <- glm(Survived ~ . -PassengerId, data = train, family = binomial)#
summary(logModel)#
#
#Predict output#
glmProbability <- predict(logModel, newdata = test)#
summary(glmProbability)#
#
glmOutput <- ifelse(glmProbability < 0.5, 0, 1)#
table(glmOutput)#
#
#Create submission file#
submission <- data.frame(PassengerId = test$PassengerId, Survived = glmOutput)#
write.csv(submission, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/submission/logisticRegression.csv', row.names = FALSE)
rm(list = ls())#
library(randomForest)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/test.csv')#
train$Survived <- as.factor(train$Survived)#
#
#Find optimal mtry with ntree = 2000#
mtries <- tunerf(x = train[, !(colnames(train) %in% c('PassengerId', 'Survived'))], y = train$Survived, ntreeTry = 2000)
mtries <- tuneRF(x = train[, !(colnames(train) %in% c('PassengerId', 'Survived'))], y = train$Survived, ntreeTry = 2000)
optimumMtry <- 2
#Name: LogitWithLasso.R#
#Date: Oct 14th, 2015#
#Purpose: Create simple classification model on pre-processed data and predict output of test data#
#Data: Can be downloaded from - #
#https://www.kaggle.com/c/titanic/data#
#packages used: glmnet, caret#
#
rm(list = ls())#
library(glmnet)#
library(caret)#
#read pre-processed data#
train <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/train.csv')#
test <- read.csv('/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/data/test.csv')#
#
#Combine train and test datasets#
test$Survived <- NA#
all <- rbind(train, test)#
#
#Create dummy variables#
dummy <- dummyVars(~ .-Survived, data = all, levelsOnly = TRUE, fullRank = TRUE)#
dummyData <- predict(dummy, newdata = all)#
fullData <- as.data.frame(cbind(dummyData, Survived = all$Survived))#
#
#Split train and test data sets#
train <- subset(fullData, !is.na(fullData$Survived))#
test <- subset(fullData, is.na(fullData$Survived))#
test$Survived <- NULL#
train$Survived <- as.factor(train$Survived)#
#
#Find optimal value of lambda#
set.seed(10)#
x_mat = as.matrix(train[, !(colnames(train) %in% c('PassengerId', 'Survived'))])#
cv.glmnet(x = x_mat, y = train$Survived, family = 'binomial', type.measure = 'class', nfolds = 10)#
minLambda <- 0.0009162309#
#
#Build logistic regression with lasso #
lassoModel <- glmnet(x = x_mat, y = train$Survived, family = 'binomial', alpha = 1, lambda = 0.0009162309)#
print(lassoModel)#
#
#Predict on test data#
x_mat_test <- as.matrix(test[, colnames(test) != 'PassengerId'])#
lassoPredictions <- predict(lassoModel, x_mat_test, type = 'response')#
summary(lassoPredictions)#
#
lassoOutput <- ifelse(lassoPredictions < 0.5, 0, 1)#
table(lassoOutput)
str(lassoOutput)
head(as.vector(lassoOutput))
submission <- data.frame(PassengerId = test$PassengerId, Survived = as.vector(lassoOutput))
head(submission)
write.csv(submission, '/Users/admin/Documents/DSCompetitions/Kaggle-Titanic/version3/submission/logitWithLasso.csv', row.names = FALSE)
library(cvTools)
install.packages('cvTools')
library(cvTools)
library(Metrics)
methods(Metrics)
ls(getNamespace('Metrics'), all.names = TRUE)
ce(0, 1)
ce(1, 0)
ce(0, 0)
ce(1, 1)
yp <- sample(0:2,50,replace=TRUE)
yt <- sample(0:2,50,replace=TRUE)
ce(yp, yt)
ll(yp, yt)
mae(yp, yt)
mse(yp, yt)
rmse(yp, yt)
cor(1, 1)
cor(x=c(1, 1, 1), y=c(1, 1, 1))
cor(x=c(1, 1, 1), y=c(1, 1, 2))
cor(x=c(1, 1, 3), y=c(1, 1, 2))
cor(x=c(1, 1, 2), y=c(1, 1, 2))
cor(x=c(1, 1, 2), y=c(2, 1, 1))
install.packages('akima')
install.packages(c('chron', 'lme4'))
install.packages(c('mcmc', 'odesolve', 'spdep', 'spastat', 'tree'))
library(MASS)
attach(bacteria)
str(bacteria)
head(bacteria)
fix(bacteria)
objects()
search()
library(help = cvTools)
library(help=xgBoost)
library(xgBoost)
library(help = xgboost)
library(xgboost)
library(help = xgboost)
exp(-1)
exp(1)
exp(-100000000)
exp(100000000)
getwd()
setwd('/Users/admin/Documents/GitHubCode/MLForHackersCode/Ch07-Optimization-BreakingCodes/')
getwd()
head(read.csv('/data/01_heights_weights_genders.csv'))
head(read.csv('data/01_heights_weights_genders.csv'))
height.weight <- read.csv('data/01_heights_weights_genders.csv')#
#
#function to get predicted weight given height, intercept a and slope b#
height.to.weight <- function(height, a, b){#
	weight <- a + b * height#
	return(weight)#
}
#function to compute squared error #
squared.error <- function(height.weight, a, b){#
	predicted_weight <- height.to.weight(height.to.weight$Height, a, b)#
	error <- height.to.weight$Height - predicted_weight#
	return (sum(error ^ 2))#
}
#Try to find optimal values of a and b within a given range#
for (a in seq(-1, 1, by = 1)){#
	for (b in seq(-1, 1, by = 1)){#
		print (squared.error(height.weight, a, b))#
	}#
}
squared.error <- function(height.weight, a, b){#
	predicted_weight <- height.to.weight(height.weight$Height, a, b)#
	error <- height.weight$Height - predicted_weight#
	return (sum(error ^ 2))#
}
for (a in seq(-1, 1, by = 1)){#
	for (b in seq(-1, 1, by = 1)){#
		print (squared.error(height.weight, a, b))#
	}#
}
print (a, b, squared.error(height.weight, a, b))
for (a in seq(-1, 1, by = 1)){#
	for (b in seq(-1, 1, by = 1)){#
		print (a, b, squared.error(height.weight, a, b))#
	}#
}
for (a in seq(-1, 1, by = 1)){#
	for (b in seq(-1, 1, by = 1)){#
		return (a, b, squared.error(height.weight, a, b))#
	}#
}
for (a in seq(-1, 1, by = 1)){#
	for (b in seq(-1, 1, by = 1)){#
		print (squared.error(height.weight, a, b))#
	}#
}
squared.error <- function(height.weight, a, b){#
	predicted_weight <- height.to.weight(height.weight$Height, a, b)#
	error <- height.weight$Weight - predicted_weight#
	return (sum(error ^ 2))#
}
for (a in seq(-1, 1, by = 1)){#
	for (b in seq(-1, 1, by = 1)){#
		print (squared.error(height.weight, a, b))#
	}#
}
optim(c(0, 0), function(x){squared.error(height.weight, x[1], x[2])})
a.error <- function (a){#
	return (squared.error(height.weight, a, 0))#
}
sapply(x, function (a) {a.error(a)})
x
a.error(a)})
sapply(x, function (a) {print(a)
a.error(a)})
curve(sapply(x, function (a) {a.error(a)}), from = -1000, to = 1000)
b.error <- function(b){#
	return (squared.error(height.weight, 0, b))#
}
curve(sapply(x, function(b) {b.error(b)}), from = -1000, to = 1000)
ridge.error <- function(height.weight, a, b, lambda){#
	predicted_weight <- height.to.weight(height.weight$Height, a, b)#
	error <- predicted_weight - height.weight$Weight#
	return (sum(error ^ 2) + lambda * (a ^ 2 + b ^ 2))#
}
optim(c(0, 0), function(x){ridge.error(height.weight, x[1], x[2], lambda)})
lambda <- 1 #assume this is the best value of lambda from CV
optim(c(0, 0), function(x){ridge.error(height.weight, x[1], x[2], lambda)})
curve(sapply(x, function(a, lambda) {a.ridgeError(a, lambda)}), from = -1000, to = 1000)
a.ridgeError <- function(a, lambda){#
	return (ridge.error(height.weight, a, 0, lambda))#
}
curve(sapply(x, function(a, lambda) {a.ridgeError(a, lambda)}), from = -1000, to = 1000)
lambda <- 1 #assume this is the best value of lambda from CV
optim(c(0, 0), function(x){ridge.error(height.weight, x[1], x[2], lambda)})
a.ridgeError <- function(a, lambda){#
	return (ridge.error(height.weight, a, 0, lambda))#
}
curve(sapply(x, function(a, lambda) {a.ridgeError(a, lambda)}), from = -1000, to = 1000)
a.ridgeError <- function(a, lambda){#
	return (ridge.error(height.weight, a, 0, lambda))#
}
curve(sapply(x, function(a, lambda) {a.ridgeError(a, lambda)}), from = -1000, to = 1000)
lambda
curve(sapply(x, function(a) {a.ridgeError(a, lambda)}), from = -1000, to = 1000)
curve(sapply(x, function(b){b.ridgeError(b, lambda)}), from = -1000, to = 1000)
b.ridgeError <- function(b, lambda){#
	return (ridge.error(height.weight, 0, b, lambda))#
}
curve(sapply(x, function(b){b.ridgeError(b, lambda)}), from = -1000, to = 1000)
absolute.error <- function(height.weight, a, b){#
	predicted_weight <- height.to.weight(height.weight$Height, a, b)#
	error <- predicted_weight - height.weight$Weight#
	return (sum(abs(error)))#
}
a.absoluteError <- function(a){#
	return (absolute.error(height.weight, a, 0))#
}
curve(sapply(x, function(a){a.absoluteError(a)}), from = -1000, to = 1000)
b.absoluteError <- function(b){#
	return (absolute.error(height.weight, 0, b))#
}
curve(sapply(x, function(b){b.absoluteError(b)}), from = -1000, to = 1000)
english.letters <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z')
length(english.letters)
caesar.cipher <- list()#
inv.caesar.cipher <- list()
for (i in 1:length(english.letters)){#
	caesar.cipher[[english.letters[i]]] <- english.letters[i %% 26 + 1]#
	inv.caesar.cipher[[english.letters[i %% 26 + 1]]] <- english.letters[i]#
}
print (caesar.cipher)
apply.cipher.to.string <- function(string, cipher){#
	output <- ''#
	for (i in 1:nchar(string)){#
		output <- paste(output, cipher[[substr(string, i, i)]], sep = '')#
	}#
	return (output)#
}#
#
apply.cipher.to.text <- function(text, cipher) {#
	output <- c()#
	for (string in text){#
		output <- c(output, apply.cipher.to.string(string, cipher))#
	}#
	return (output)	#
}
apply.cipher.to.text (c('sample', 'text'), caesar.cipher)
sample(1:length(english.letters), length(english.letters))
caesar.cipher
names(caesar.cipher)
apply.cipher.to.text (c('sample', 'text'), caesar.cipher)
#function to generate random cipher#
generate.random.cipher <- function(){#
	cipher <- list()#
	inputs <- english.letters#
	outputs <- english.letters[sample(1:length(english.letters), length(english.letters))]#
	for (i in 1:length(english.letters)){#
		cipher[[inputs[i]]] <- outputs[i]#
	}#
	return (cipher)#
}#
#
modify.cipher <- function(cipher, input, output){#
	new.cipher <- cipher#
	new.cipher[[input]] <- output#
	old.output <- cipher[[input]]#
	collateral.input <- names(which(sapply(names(cipher), function(key){ cipher[[key]]}) == output))#
	new.cipher[[collateral.input]] <- old.output#
	return (new.cipher)#
}#
#
propose.modified.cipher <- function(cipher){#
	input <- sample(names(cipher), 1)#
	output <- sample(english.letters, 1)#
	return (modify.cipher(cipher, input, output))#
}
db <- load('data/lexical_database.Rdata')
str(df)
str(db)
.load('data/lexical_database.Rdata')
load('data/lexical_database.Rdata')
lexical_database[['a']]
lexical.database[['a']]
lexical.database[['a']]#
lexical.database[['the']]#
lexical.database[['he']]#
lexical.database[['she']]#
lexical.database[['his']]#
lexical.database[['her']]
one.gram.prob <- function(one.gram, lexical.database = list()){#
	lexical.prob <- lexical.database[[one.gram]]#
	if (is.null(lexical.prob) || is.na(lexical.prob) ){#
		return (.Machine$double.eps)#
	}#
	else{#
		return (lexical.prob)#
	}#
}
.Machine$double.eps
one.gram.prob <- function(one.gram, lexical.database = list()){#
	lexical.prob <- lexical.database[[one.gram]]#
	if (is.null(lexical.prob) || is.na(lexical.prob) ){#
		return (.Machine$double.eps)#
	}#
	else{#
		return (lexical.prob)#
	}#
}
log.prob.text <- function(text, cipher, lexical.database = list()){#
	log.prob <- 0.0#
	for (one.gram in text){#
		decrypted.word <- apply.cipher.to.string(one.gram, cipher)#
		log.prob <- log.prob + one.gram.prob(one.gram, lexical.database)#
	}#
	return (log.prob)#
}
runif(1)
?runif()
metropolis.step <- function(text, cipher, lexical.database = list()){#
	proposed.cipher <- propose.modified.cipher(cipher)#
	log.prob1 <- log.prob.text(text, cipher, lexical.database)#
	log.prob2 <- log.prob.text(text, proposed.cipher, lexical.database)#
	if (log.prob2 > log.prob1){#
		return (proposed.cipher)#
	}#
	else {#
		a <- exp(log.prob2 - log.prob1)#
		x <- runif(1)#
		if (x < a){#
			return (proposed.cipher)#
		}#
		else{#
			return (cipher)#
		}#
	}#
}
decrypted.text <- c('here', 'is', 'a', 'sample', 'text')#
encrypted.text <- apply.cipher.to.text(decrypted.text, caesar.cipher)
set.seed(1)#
cipher <- generate.random.cipher()#
restults <- data.frame()#
iter <- 500
for (i in 1:iter){#
	log.prob <- log.prob.text(encrypted.text, cipher, lexical.database)#
	current.decrypted.text <- paste(apply.cipher.to.text(encrypted.text, cipher), collapse = ' ')#
	correct.text <- as.numeric(current.decrypted.text == paste(decrypted.text, collapse = ' '))#
	results <- rbind(results, data.frame(Iteration = i, logProb = log.prob, currentDecryptedText = current.decrypted.text, correctText = correct.text))#
	cipher <- metropolis.step(encrypted.text, cipher, lexical.database)#
}
set.seed(1)#
cipher <- generate.random.cipher()#
results <- data.frame()
for (i in 1:iter){#
	log.prob <- log.prob.text(encrypted.text, cipher, lexical.database)#
	current.decrypted.text <- paste(apply.cipher.to.text(encrypted.text, cipher), collapse = ' ')#
	correct.text <- as.numeric(current.decrypted.text == paste(decrypted.text, collapse = ' '))#
	results <- rbind(results, data.frame(Iteration = i, logProb = log.prob, currentDecryptedText = current.decrypted.text, correctText = correct.text))#
	cipher <- metropolis.step(encrypted.text, cipher, lexical.database)#
}
write.table(results, file = 'data/results.csv', row.names = FALSE, sep = '\t')
